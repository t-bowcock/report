\section{Introduction}
\section{Tools}
\subsection{IDE}
Visual Studio Code was used for this project due to prior experience with the IDE. It is also one of the most popular tools in the industry 
as shown by the Stack Overflow Developer Survey\cite{StackOverflowDeveloper} and the TOP IDE Index\cite{TOPIDETop}.
VS Code has excellent support for many programming languages, and with a wealth of community made extensions there are many tools to aid with development.
\subsection{Version Control}
Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later.\cite{GitVersionControl}
Using version control with an external hosting provider also makes working over several computers easy which will be useful for this project, and it ensures everything is backed up remotely.
Git was chosen for version control as it is the industry standard, with GitHub being used as the hosting provider.
\subsection{Database Visualisation}
Neo4j has two tools for database visualisation as part of the AuraDB web interface; Bloom and Browser. Bloom is used to 
visualise the data in a graph as has been discussed in the design chapter previously. Browser is used to test CYPHER queries on the database.
CYPHER is the query language created by Neo4j for retrieving data from their graph databases. As shown in the screenshot below, the user can enter a 
query and have the data returned as a graph, table (represented as a series of JSON objects), raw text and as code (JSON objects).
This is useful for quickly testing CYPHER queries and debugging database interactions.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{neo4jBrowser}
    \caption{AuraDB Browser}
\end{figure}
\subsection{Libraries}
\subsection{Client Side}
\begin{description}
    \item[Cytoscape] - A JavaScript library that `allows you to easily display and manipulate rich, interactive graphs'\cite{franzCytoscapeJsGraph2016}. 
    Used in this project to display the data in graphs to the user.
    \item[Material] - An Angular specific library containing material design componenets, used in this project to quickly create UI componenets.
\end{description}
\subsection{Server Side}
\begin{description}
    \item[Neomodel] - An Object Graph Mapper (OGM) for the Neo4j graph database\cite{NeomodelDocumentationNeomodel}, used to define Django models for access database data.
    \item[django-cors-headers] - A Django App that adds Cross-Origin Resource Sharing (CORS) headers to responses. This allows in-browser requests to your Django application from other origins.\cite{yiuDjangocorsheadersDjangocorsheadersDjango} 
    this is required for the front end and back end to communicate properly.
\end{description}
\section{Data Processing}
The first major section of the implementation was setting up the graph database and getting the data required to do so.
\subsection{Finding Data Source}
The initial project brief suggested using the Fandom wiki for the data source as Fandom wikis have the option for downloading 
an XML dump from the `Special:Statistics' page. As discussed in the background chapter, the Fandom wiki is also the most comprehensive 
source of data about the game, especially regarding the item interactions. The XML dumps are not always kept up to date, so while 
a new dump was being requested other options for the data source were investigated. This included investigating the game files, where 
all the resource files have been packed in to `A' files. In the folder containing the resource files there is a readme which explains 
that this was done to prevent spoilers and any secrets being found through the files. A resource extractor tool is included with later versions of the game, 
however the files do not contain any information regarding the interactions of items.

Once received, the updated XML dump presented its own challenges, the first being its size, at just under 500,000 lines long and around 20MB it was too 
large for most text editors to load with syntax highlighting. This made it difficult to understand the structure of the data as the XML tags became harder 
to pick out from regular text. Aside from a small preamble, all the data in the file is contained in a series of `page' elements, which unsurprisingly represents a page on the site.
Each page element has a similar structure to the below example.
\begin{verbatim}
    <page>
    <title>Template:â€¢</title>
    <ns>10</ns>
    <id>161</id>
    <redirect title="Template:*" />
    <revision>
      <id>161</id>
      <timestamp>2014-09-16T20:27:12Z</timestamp>
      <contributor>
        <username>Maintenance script-gpuser</username>
        <id>41555837</id>
      </contributor>
      <comment>&amp;lt;default import&amp;gt;</comment>
      <origin>161</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24" sha1="2dwqfi7oey6311xkwxu3dhjasn8gfsi" xml:space="preserve">#redirect [[Template:*]]</text>
      <sha1>2dwqfi7oey6311xkwxu3dhjasn8gfsi</sha1>
    </revision>
  </page>
\end{verbatim}

The important parts to note from this is that the title is the web page title and the text element contains the data to be displayed (or in this case a redirect to another page).
There doesn't appear to be a particular order to the pages and due to the formatting of the file it can be hard to tell where one page ends and another begins.
\subsection{Data Extraction}
while looking for alternative sources and waiting on response from wiki admins worked on extracting the data
used beautifulsoup to parse the xml tags and traverse the data
Used the items collection page to get the names of all the items locate the correct xml elements
trinkets and characters(?) had to be hardcoded as a complete list didnt exist in the xml

beautifulsoup is used to traverse the data and get the 'text' child of all the relevant 'pages'
then regex is used to grab the relevant sections from the text
\subsection{Cleaning the Data}
regex is used again to clean the sections of text
The text contains lots of tags for vairous features in the wiki such as dlc tags and links to other pages
turns bullet pointed lists into nested arrays
\subsection{Importing into Database}
neo4j has a data importer tool which uses csv files to populate a data model
csv files were generated from the data objects in python
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{dataImport}
    \caption{Database Model in Neo4j Data Importer}
\end{figure}
\section{Web Stack}
creating django project with an api app and a main "query" app
api app contains webserver (?) and the settings
each app then contains the the models, views and urls etc
urls.py defines what functions get called by each urls
views.py defines the functions used by urls, these functions contain all the functionality the user interacts with,
each function takes in a request from the url and depending on the type of request (e.g get, post etc), performs the relevant action
models.py contains classes each of which represent a type if data entity

create a angular project and any components
(explain angular project structure)
each component represents a "screen"
(explain component structure)

There is a shared services file, this defines the interaction with the backend api
the address of the backend server is defined in there and the functions that make the http requests

each project has it's own git repo which are then submoduled into the main project repo
\section{Database Interaction}
Used neomodel to define classes for each entity and relationship type, these classes contain methods for retrieving data
Getting all nodes is very easy due to built in methods, however these do not exist for relationships
so to get all relationships you have to perform a cypher query to get them
\section{Client Side}
first made tables to check everything worked
found that the json like object created by neomodel couldn't be interpreted by the front, so had to manually make a get method for each class
This causes getting large amounts of data (relationships) to take quite a long time

I created a second component to show the data in a graph form as per my initial design
To do this I used cytoscape.js (more info)
had to rewrite how the data is formatted when sent to the front end so that it matchs what cytoscape expects
still takes a long time to get data from backend 
so decided to write it to a json file so at least for quicker debugging it can load directly from the file
tweaked some settings to make the graph easier to read

\section{Conclusion}