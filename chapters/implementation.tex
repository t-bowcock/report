\section{Introduction}
\section{Tools}
\subsection{IDE}
Visual Studio Code was used for this project due to prior experience with the IDE. It is also one of the most popular tools in the industry 
as shown by the Stack Overflow Developer Survey\cite{StackOverflowDeveloper} and the TOP IDE Index\cite{TOPIDETop}.
VS Code has excellent support for many programming languages, and with a wealth of community made extensions there are many tools to aid with development.
\subsection{Version Control}
Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later.\cite{GitVersionControl}
Using version control with an external hosting provider also makes working over several computers easy which will be useful for this project, and it ensures everything is backed up remotely.
Git was chosen for version control as it is the industry standard, with GitHub being used as the hosting provider.
\subsection{Database Visualisation}
Neo4j has two tools for database visualisation as part of the AuraDB web interface; Bloom and Browser. Bloom is used to 
visualise the data in a graph as has been discussed in the design chapter previously. Browser is used to test CYPHER queries on the database.
CYPHER is the query language created by Neo4j for retrieving data from their graph databases. As shown Figure 4.1, the user can enter a 
query and have the data returned as a graph, table (represented as a series of JSON objects), raw text and as code (JSON objects).
This is useful for quickly testing CYPHER queries and debugging database interactions.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{neo4jBrowser}
    \caption{AuraDB Browser}
\end{figure}
\section{Libraries}
\subsection{Data Processing}
\begin{description}
    \item[Beautiful Soup] - A Python library for pulling data out of HTML and XML files\cite{BeautifulSoupDocumentation}. 
    Used to parse the XML dump and extract data for the database.
\end{description}
\subsection{Client Side}
\begin{description}
    \item[Cytoscape] - A JavaScript library that `allows you to easily display and manipulate rich, interactive graphs'\cite{franzCytoscapeJsGraph2016}. 
    Used in this project to display the data in graphs to the user.
    \item[Material] - An Angular specific library containing material design components, used in this project to quickly create UI components.
\end{description}
\subsection{Server Side}
\begin{description}
    \item[Neomodel] - An Object Graph Mapper (OGM) for the Neo4j graph database\cite{NeomodelDocumentationNeomodel}, used to define Django models for access database data.
    \item[django-cors-headers] - A Django App that adds Cross-Origin Resource Sharing (CORS) headers to responses. This allows in-browser requests to your Django application from other origins.\cite{yiuDjangocorsheadersDjangocorsheadersDjango} 
    this is required for the front end and back end to communicate properly.
\end{description}
\section{Data Processing}
The first major section of the implementation was setting up the graph database and getting the data required to do so.
\subsection{Finding Data Source}
The initial project brief suggested using the Fandom wiki for the data source as Fandom wikis have the option for downloading 
an XML dump from the \verb|Special:Statistics| page. As discussed in section 2.3, the Fandom wiki is also the most comprehensive 
source of data about the game, especially regarding the item interactions. The XML dumps are not always kept up to date, so while 
a new dump was being requested other options for the data source were investigated. This included investigating the game files, where 
all the resource files have been packed in to \verb|A| files. In the folder containing the resource files there is a readme which explains 
that this was done to prevent spoilers and any secrets being found through the files. A resource extractor tool is included with later versions of the game, 
however the files do not contain any information regarding the interactions of items.\par
Once received, the updated XML dump presented its own challenges, the first being its size, at just under 500,000 lines long and around 20MB it was too 
large for most text editors to load with syntax highlighting. This made it difficult to understand the structure of the data as the XML tags became harder 
to pick out from regular text. Aside from a small preamble, all the data in the file is contained in a series of \verb|page| elements, which unsurprisingly represents a page on the site.
Each page element has a similar structure to the below example (Fig. 4.2).
\begin{figure}[H]
    \begin{lstlisting}[language=XML]
        <page>
        <title>Template:</title>
        <ns>10</ns>
        <id>161</id>
        <redirect title="Template:*" />
        <revision>
          <id>161</id>
          <timestamp>2014-09-16T20:27:12Z</timestamp>
          <contributor>
            <username>Maintenance script-gpuser</username>
            <id>41555837</id>
          </contributor>
          <comment>&amp;lt;default import&amp;gt;</comment>
          <origin>161</origin>
          <model>wikitext</model>
          <format>text/x-wiki</format>
          <text bytes="24" sha1="2dwqfi7oey6311xkwxu3dhjasn8gfsi" xml:space="preserve">#redirect [[Template:*]]</text>
          <sha1>2dwqfi7oey6311xkwxu3dhjasn8gfsi</sha1>
        </revision>
      </page>
    \end{lstlisting}
    \caption{Example page format}
\end{figure}

The important parts to note from this is that the \verb|title| element is the web page title and the \verb|text| element contains the data to be displayed (or in this case a redirect to another page).
There doesn't appear to be a particular order to the pages and due to the formatting of the file it can be hard to tell where one page ends and another begins.
\subsection{Data Extraction}
Due to the structure of the XML data, the easiest way to extract the data is to use the title tags for each page to find the relevant and pages 
and then pull the data from the text tag. The XML file contains collections pages which each contain a list of items available in each version of the game, this can be 
used to get a list of all the item names. Character names are fetched from the \verb|Characters| page. This has a list of links to the character pages, from which the names can be extracted using RegEx. 
Unfortunately, the same does not exist for trinkets as the page that would contain that list instead uses a template which autofills the data. 
As a temporary fix, the list of trinkets is fetched from a hardcoded file.
\begin{figure}[H]
    \begin{lstlisting}[language=Python]
        ITEM_REGEX = re.compile("content =(.*?)}}", flags=re.DOTALL)

        def _get_item_names(self):
            collection = self.soup.find("title", text="Collection Page (Repentance)").find_parent("page").find("text").text
            return [
                x.strip()
                for x in ITEM_REGEX.search(collection)[1].replace("\n", "").replace("Number Two", "No. 2").split(",")
            ]
    \end{lstlisting}
    \caption{Function that returns the list of item names}
\end{figure}

Figure 4.3 shows the code used for getting the list of item names, similar code has been used for getting character names.
\verb|re| is a Python library that provides regular expression matching operations. In the data extraction script a series of 
compiled RegEx statements are defined as constants, \verb|ITEM_REGEX| being one of them. These are used through the script 
to pull data from the XML file based on the format of the data. Here BeautifulSoup is used to find the title tag that contains the text 
`Collection Page (Repentance)', this is then used to get its parent and to search that for the text element. 
To get the data from the string of text the RegEx statement is used to search for a string that starts with \verb|content =| and ends with \verb|}}|.
The \verb|(.*?)| matches a string of any characters of any length non-greedily and puts it in a match group. 
This means it will match against any character until the first instance of \verb|}}| and the object returned is sub-scriptable to get the match groups.
Accessing group 0 is the whole match which will included the \verb|content =| and \verb|}}|, accessing group 1 is the first (and in this case only) match group.
The string returned by accessing the match group then has new line characters removed and and is split into a list using the commas.
Each value in this array then has any leading and trailing whitespace removed using \verb|strip()| and a list comprehension.
There is an extra step needed here due to the nature of the XML data and that is manually replacing the item name for `Number Two'.
This had to be done as the name appears in several formats throughout the data, but the only one that has a page with data in is `No. 2'. 
The rest of the pages just redirect back to that page and so instead of handling the redirect it is easier to catch and replace the different formats when they appear.

Functions could now be written for getting item, trinket and character data respectively. These functions all follow the same format 
and the differences are in catching special exceptions and the output created.
\begin{figure}[H]
    \begin{lstlisting}[language=Python]
        def get_all_items(self) -> list:
        item_names = self._get_item_names()
        tags = self.soup.find_all("title")
        item_data = []
        for tag in tags:
            if tag.text not in item_names:
                continue

            item_text = tag.find_parent("page").find("text").text

            item_id = self._infobox_get(item_text, ID_REGEX)
            if item_id is None:
                continue

            item_data.append(
                [
                    tag.text,
                    f"I{item_id}",
                    self._infobox_get(item_text, QUOTE_REGEX),
                    self._infobox_get(item_text, DESCRIPTION_REGEX),
                    self._infobox_get(item_text, ITEM_QUALITY_REGEX),
                    self._infobox_get(item_text, UNLOCK_REGEX),
                    self._list_get(item_text, EFFECTS_REGEX, True),
                    self._list_get(item_text, NOTES_REGEX, True),
                ]
            )
            self.id_lookup[tag.text.lower()] = f"I{item_id}"
            self.synergies[tag.text] = self._list_get(item_text, SYNERGIES_REGEX)
            self.interactions[tag.text] = self._list_get(item_text, INTERACTIONS_REGEX)
        return item_data
    \end{lstlisting}
    \caption{Function that extracts the item data}
\end{figure}
Each function iterates over the title tags in the XML file and checks if it is in the list of tags being looked for. 
If it is the text element belonging to that title tag is fetched and an attempt at getting the item ID is made. This is done 
by passing the relevant compiled RegEx statement and the text to process to a function which will execute the RegEx and return \verb|None| 
if no match is found and the ID string if a match is found. If no ID is found that means the page found isn't actually an item page and so can be ignored.
The rest of the data can then be extracted using the same method as for getting the ID, the only difference being for the effects and notes data another function 
is used that has extra processing for handling lists in the text. The synergy and interaction data is added to a separate dictionary as it requires extra processing 
to create usable data. The item name and ID is added to a lookup dictionary which will be used later when processing the synergies/interactions.
Importantly, for each ID a letter prefix is added to indicate whether the ID belongs to an item, trinket or character; this is needed to ensure the IDs 
are unique across the entire database.

The function for getting character data is much simpler than for items and trinkets, this is because it only gets the character name and ID. 
The decision was made to ignore character data because it is much more complicated than any of the other data types. The page for each character has 
a unique structure, especially for characters that are a combination of two characters such as Jacob and Esau or The Forgotten. Combined with the fact that 
most of the character data does not contribute to the relationships in the database which is the main focus, it was decided that processing the character data was outside 
the scope of this project. 

Once all the item, character, and trinket data has been extracted the lookup dictionaries can be used to process the synergies and interactions. 
The function to process the data is fairly simple, it iterates over each entry in the dictionary, each entry contains a list of strings. That list is iterated over 
and the RegEx show in figure 4.5 is applied to create a list of item names which represent the relationship destination. The RegEx looks for tags that link to other items/characters/trinkets,
for example {{i|Libra}}. However, these tags don't seem to always follow a set structure which is why the RegEx has some extra match groups either side of the \verb|(.+?)| group to catch and ignore extra 
characters that are not relevant. These irregularities also mean an if block is needed to catch links where the link text does not match the text in the title tag.
\begin{figure}[H]
    \begin{lstlisting}[language=Python]
        destinations = re.findall(r"{{[ict]\|(1=)?(.+?)(\|.+?)?}}", relationship[0], re.IGNORECASE)
    \end{lstlisting}
    \caption{Synergy/Interaction RegEx}
\end{figure}
The ID lookups are then used to find the ID for the source entity and the destination entity found via the RegEx. This is then added to the output along with the string containing the relationship data.
\subsection{Cleaning the Data}
The data extracted above still contains special characters and formatting that is used by the XML to generate the website correctly. 
This includes \verb|<br>| HTML tags, tags using square brackets to denote alternative text/images and the curly brace tags seen above which are used for a variety of purposes.
One of the uses of curly brace tags is to show which version of the game the information is relevant to; on the webpage this is show via a small image but in the XML
that image is represent by a tag in the following format \verb#{{dlc|<tag>}}# where \verb|<tag>| is replaced by a code that represent the game version.
These tags are replaced with a relevant string using RegEx, e.g. \verb#{{dlc|anr}}# becomes `(Added in Afterbirth, Removed in Repentance)'.
RegEx is then used to extract the useful text from the rest of the tags and remove the brackets and any other formatting characters.

The lists used when processing synergies and interactions have to be generated from the string return by the data extraction.
In the XML the bullet pointed lists are represented using `*' characters where the number of characters indicates the level of indentation.
The string is split using the bullet characters and then a recursive function is used to create a nested list that represents the list indentation.
Optionally, the function can also take the nested list and create a formatted string with tab spacing to create the indentation.
\subsection{Importing into Database}
The AuraDB platform provided by Neo4j has a data importer tool which uses CSV files to populate a predefined model. 
Creating CSV files with Python is very simple, the data needs to be in a 2D array where each inner array is a row in the CSV file,
the CSV writer also takes a list of headers and from the two it can create a CSV file. The CSV files needed to be created so that each file represents 
a node/edge in the model, this meant 5 CSV files were needed. With the files uploaded to the import tool the model shown in figure 4.6 could be created.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{dataImport}
    \caption{Database Model in Neo4j Data Importer}
\end{figure}
As the model shows items and trinkets can have interactions and relationships between each other and themselves. The relationships to character 
entities are only towards the character nodes because the character data is not processed as discussed above.
After importing the data Neo4j Bloom can be used to check the database structure which is show in figure 4.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{importedData}
    \caption{Bloom showing complete database}
\end{figure}
\section{Web Stack}
creating django project with an api app and a main "query" app
api app contains webserver (?) and the settings
each app then contains the the models, views and urls etc
urls.py defines what functions get called by each urls
views.py defines the functions used by urls, these functions contain all the functionality the user interacts with,
each function takes in a request from the url and depending on the type of request (e.g get, post etc), performs the relevant action
models.py contains classes each of which represent a type if data entity

create a angular project and any components
(explain angular project structure)
each component represents sozdfn;sergnlarejgnsl
(explain component structure)

There is a shared services file, this defines the interaction with the backend api
the address of the backend server is defined in there and the functions that make the http requests

each project has it's own git repo which are then submoduled into the main project repo
\subsection{Database Interaction}
Used neomodel to define classes for each entity and relationship type, these classes contain methods for retrieving data
Getting all nodes is very easy due to built in methods, however these do not exist for relationships
so to get all relationships you have to perform a cypher query to get them
talk about this generally as specific methods and problems will be talked about in each sectio
\subsection{Displaying Data in Tables}
first made tables to check everything worked
found that the json like object created by neomodel couldn't be interpreted by the front, so had to manually make a get method for each class
This causes getting large amounts of data (relationships) to take quite a long time
\subsection{Displaying Data in Graph}
I created a second component to show the data in a graph form as per my initial design
To do this I used cytoscape.js (more info)
had to rewrite how the data is formatted when sent to the front end so that it matchs what cytoscape expects
still takes a long time to get data from backend 
so decided to write it to a json file so at least for quicker debugging it can load directly from the file
tweaked some settings to make the graph easier to read
explain fixing the long loading issue
\subsection{Inspect Element}
writing proper async queries
overlay issue
\subsection{Searching}
\section{Conclusion}